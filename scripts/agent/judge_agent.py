from typing_extensions import override
from pydantic import BaseModel, Field

from scripts.agent.base_agent import BaseAgent
from scripts.model.language_models.openai_client import OpenAIClient


class OutputPoint(BaseModel):
    model_name: str = Field(description="The name of the model")
    score: int = Field(description="The point of the output")
    justification: str = Field(description="Explanation of the point")


class JudgeResponse(BaseModel):
    model_results: list[OutputPoint] = Field(description="The result of the model")
    overall_comparison: str = Field(description="The overall comparison of the outputs")


class JudgeAgent(BaseAgent):
    def __init__(self, client: OpenAIClient):
        super().__init__(client)

    @override
    def invoke(
        self,
        user_request: str,
        response_json: bool = True,
        response_model: type[BaseModel] = JudgeResponse,
    ):
        prompt = f"""
        # AI as a Judge Prompt

        ## ROLE
        You are an impartial AI Judge that evaluates and compares the output quality of different algorithms given the same input query. 
        You do not generate new answers; instead, you analyze and score the provided outputs for their relevance, correctness, completeness, and explainability.

        ## INPUT
        You will receive:
        - The original user query or task description.
        - Multiple outputs, each generated by a different algorithm (e.g., MyCustomGraphRAG, ClassicGraphRAG, KNN, PageRank-based GraphRAG).
        - Any relevant context about the algorithms if provided.

        ## TASK
        For each algorithm output:
        1. Assign an overall score from 0 to 10 based on:
        - Relevance: How well does the output answer the original query?
        - Correctness: Are there factual errors?  
        - Completeness: Is the information sufficiently detailed?
        - Explainability: Is the answer coherent and easy to follow?

        2. Provide a short justification (2â€“4 sentences) explaining the score.
        3. After evaluating all outputs individually, write a brief comparison stating:
        - Which algorithm performed best and why.
        - Any specific strengths or weaknesses you noticed.

        ## OUTPUT FORMAT
        Your output must be in JSON format.
        {response_model(
            model_results=[
                OutputPoint(model_name="Pagerank-based GraphRAG", score=1, justification="Detailed explanation here"),
                OutputPoint(model_name="KNN-based GraphRAG", score=1, justification="Detailed explanation here"),
                OutputPoint(model_name="GraphRAG-based", score=1, justification="Detailed explanation here"),
            ],
            overall_comparison="Brief comparison summary",
        ).model_dump_json()}

        ## RULES
        - Your overall comparison must be well detailed and must be in a way that is easy to understand.
        - Justifications must be well explained and must be in a way that is easy to understand.
        - Be completely objective and consistent - evaluate only the quality of the final answer.
        - Do not generate new answers; judge only what is provided.
        - Highlight both strong and weak points for each output.
        - Focus solely on the answer quality, not the underlying methodology.
        - Evaluate each output in isolation based purely on its merit as a response.

        ### Input to Judge:
        {user_request}
        """
        return self.client.generate_text(prompt, response_json=response_json)
