import json
from scripts.evaluation_pipeline import AcademicEvaluationPipeline


def test_multiple_runs():
    """Test the multiple runs + averaging system with Central Limit Theorem"""

    print("ðŸ§® Testing Central Limit Theorem Application to AI Judge")
    print("=" * 60)
    print("ðŸ“ Theory: As nâ†’âˆž, sample mean approaches population mean")
    print("ðŸŽ¯ Goal: Reduce AI judge variance through statistical averaging")
    print()

    # Initialize with 5 runs (good balance of reliability vs speed)
    pipeline = AcademicEvaluationPipeline(n_runs=5)

    # Test question
    test_question = "What is object-oriented programming?"

    print(f"â“ Test Question: {test_question}")
    print()

    # Compare all methods with multiple runs
    comparison = pipeline.compare_methods(test_question)

    if comparison:
        print(f"\nðŸ† FINAL RESULTS (Central Limit Theorem Applied)")
        print("=" * 50)

        for result in comparison.results:
            variance_pct = (
                (result.score_variance / result.overall_score * 100)
                if result.overall_score > 0
                else 0
            )
            reliability = (
                "High" if variance_pct < 5 else "Medium" if variance_pct < 10 else "Low"
            )

            print(f"\nðŸ“Š {result.method_name}:")
            print(f"   ðŸŽ¯ Final Score: {result.overall_score:.3f}")
            print(f"   ðŸ“ˆ Standard Dev: Â±{result.score_variance:.3f}")
            print(f"   ðŸ“Š Variance: {variance_pct:.1f}%")
            print(f"   ðŸ”¢ Sample Size: {result.run_count} runs")
            print(f"   âœ… Reliability: {reliability}")
            print(
                f"   ðŸŽ² Individual Scores: {[f'{s:.3f}' for s in result.individual_scores]}"
            )

        print(f"\nðŸ† Winner: {comparison.best_method}")
        print(f"ðŸ“Š This result is statistically robust due to multiple runs!")

        # Show before/after comparison
        print(f"\nðŸ”¬ VARIANCE ANALYSIS:")
        print("Without CLT: Single run â†’ high variance â†’ unreliable")
        print("With CLT: Multiple runs â†’ reduced variance â†’ reliable")
        print(
            f"ðŸŽ¯ Improvement: âˆš{pipeline.n_runs} = {pipeline.n_runs**0.5:.1f}x variance reduction!"
        )

    else:
        print("âŒ No results obtained")


if __name__ == "__main__":
    test_multiple_runs()
